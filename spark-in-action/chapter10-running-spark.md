# 10장 스파크 클러스터 구동

* 스파크 클러스터를 구축하고 사용하는 방법을 설명
* 스파크 클러스터는 _복수의 머신에서 분산된 방식으로 실행되는 프로세스들의 연결된 집합_ 으로 볼 수 있음

이 장에서 배우는 것

* 클러스터의 유형
  * YARN
  * MESOS
  * 스파크 자체 클러스터
  * 스파크 로컬 모드도 가능 \(+로컬 클러스터 모드\)
    * 주로 테스트 용도로 사용
    * 로컬 모드: 단일 머신에서 실행되는 일종의 가상 클러스터
    * 로컬 클러스터 모드: 단일 머신에서 제한적으로 실행되는 스파크 자체 클러스터
* 모든 클러스터 유형에 통용되는 스파크 런타임 아키텍처의 컴포넌트
  * 스파크 컨텍스트 및 스케쥴러
  * 드라이버 프로세스, 실행자 프로세스
  * 잡 및 리소스 스케쥴링
  * 스파크 웹 UI \(스파크 잡을 모니터링하는 데 활용\)
* 스파크 런타임 인스턴스의 설정 방법\(모든 클러스터 유형에서 거의 유사\)

## 10.1 스파크 런타임 아키텍처의 개요

다양한 스파크 클러스터 유형의 세세한 특성 대신 모든 유형에서 공통으로 사용하는 컴포넌트를 설명

### 10.1.1 스파크 런타임 컴포넌트

스파크 런타임 컴포넌트를 잘 알아두면 스파크 잡이 어떻게 작동하는지 이해하는 데 도움이 됨

* client
* driver
* executor

드라이버 프로세스와 실행자 프로세스의 실제 물리적 위치는 클러스터 요형과 설정에 따라 다르다. 같은 머신일 수도 있고 다른 머신일 수도 있음

#### 10.1.1.1 클라이언트 프로세스의 역할

* 아래와 같은 드라이버 프로그램들을 시작
  * spark-submit 스크립트
  * spark-shell 스크립트
  * 스파크 API를 사용한 커스텀 애플리케이션
* 스파크 애플리케이션에 필요한 classpath와 모든 설정 옵션 준비
* 주어진 애플리케이션 인수 값을 드라이버에서 실행될 스파크 애플리케이션에 전달

#### 10.1.1.2 드라이버의 역할

* 스파크 애플리케이션의 실행을 관장하고 모니터링
* 하나의 스파크 애플리케이션에는 드라이버가 _오직 하나만_ 존재
* 스파크 애플리케이션을 감싸는 일종의 래퍼 역할을 한다고도 볼 수 있음
* 드라이버와 그 하위 컴포넌트\(스파크 컨텍스트와 스케쥴러\)의 담당 역할
  * 클러스터 매니저에 메모리 및 CPU 리소스 요청
  * 애플리케이션 로직을 스테이지와 태스크로 분할\(4장 참고\)
  * 여러 실행자에 태스크를 전달
  * 태스크 실행 결과를 수집

드라이버 프로그램의 실행 방법 두 가지

* 클러스터 배포 모드\(cluster-deploy mode\): 드라이버 프로세스를 _클러스터 내부에서 별도의 JVM 프로세스로 실행_, 드라이버 프로세스의 리소스(주로 JVM 힙 메모리)를 클러스터가 관리
* 클라이언트 배포 모드\(client-deploy mode\): 드라이버를 _클라이언트의 JVM 프로세스에서 실행_, 클러스터가 관리하는 실행자들과 통신

배포 모드에 따라 스파크를 설정하는 방법과 클라이언트 JVM에 필요한 리소스 요구량이 달라질 수 있다.

#### 10.1.1.3 실행자의 역할

* 드라이버가 요청한 태스크들을 받아서 실행하고, 그 결과를 드라이버로 반환하는 JVM 프로세스
* 태스크들은 자신이 가진 여러 태스크 슬롯에서 병렬로 실행
* 일반적으로 태스크 슬롯 개수는 CPU 코어 개수의 두세 배 정도로 설정
* 간혹 태스크 슬롯을 CPU 코어라고 지칭하는 경우도 있는데, 엄밀히 말해 태스크 슬롯은 스레드로 구현되므로 머신의 CPU 코어 개수와 반드시 일치할 필요는 없음

#### 10.1.1.4 스파크 컨텍스트의 생성

* 드라이버는 `SparkContext` 인스턴스를 생성하고 시작
  * 스파크 REPL 셸은 드라이버 프로그램 역할을 하면서 미리 설정된 `SparkContext`를 `sc`라는 변수로 제공
  * JAR 파일을 제출하거나 스파크 API로 스파크 독립형 애플리케이션을 실행할 때는 직접 스파크 컨텍스트를 생성해야 함
* `SparkContext`는 JVM당 하나만 생성 가능
  * `spark.driver.allowMultipleContexts`라는 옵션이 있긴 하지만 여러 개의 `SparkContext` 사용은 권장하지 않음
* `SparkContext`는 RDD를 생성하거나 데이터를 로드하는 등 다양한 작업을 수행하는 여러 유용한 메서드를 제공하며, 스파크 런타임 인스턴스에 접근할 수 있는 기본 인터페이스

### 10.1.2 스파크 클러스터 유형

스파크 클러스터 유형마다 고유의 특징과 장단이 있으므로 환경과 활용 사례에 적합한 클러스터를 사용해야 함

#### 10.1.2.1 스파크 자체 클러스터

* 스파크 전용 클러스터로 오직 스파크 애플리케이션에만 적합하도록 설계됨
  * Kerberos 인증 프로토콜로 보호된 HDFS 지원 X
* 잡 시작에 걸리는 시간이 YARN보다 짧음

#### 10.1.2.2 YARN 클러스터

* YARN은 하둡의 리소스 매니저 및 작업 실행 시스템
  * 하둡 버전1의 맵리듀스\(MapReduce\) 엔진을 대체한 것으로 맵리듀스 버전2라고도 부름
* YARN을 사용했을 때의 장점
  * 이미 대규모로 YARN 클러스터를 운영하는 조직이 많음
  * 스파크 뿐만 아니라 다양한 유형의 자바 애플리케이션 실행 가능
    * 즉, 기존 레거시 하둡과 스파크 애플리케이션을 손쉽게 통합 가능
  * 서로 다른 사용자 및 조직의 애플리케이션을 격리하고 우선순위 조정 가능
  * Kerberos를 기반으로 하는 보안 HDFS는 오직 YARN에서만 지원
  * 스파크를 클러스터의 모든 노드에 설치할 필요 X

#### 10.1.2.3. 메소스 클러스터

* 확장성과 장애 내성을 갖춘 C++ 기반 분산 시스템 커널
* 2단계 스케줄링 아키텍처로 구성되어서 _스케줄러 프레임워크의 스케줄러_ 라고도 부름
* 메소스를 사용했을 때의 장점
  * YARN과 달리 C++과 파이썬 애플리케이션을 지원
  * YARN이나 스파크 자체 클러스터는 메모리 리소스만 스케줄링할 수 있지만 메소스는 CPU, 디스크 ㄷ공간, 네트워크 포트 등 다양한 유형의 리소스 스케줄링 가능
  * 다른 클러스터에서 지원하지 않는 다양한 추가 옵션 제공
* YARN과 메소스 중 어떤 것이 더 나은지에 대해서는 논쟁이 있음
  * [Myriad](http://myriad.incubator.apache.org/)를 사용하면 YARN을 메소스 프레임워크 형태로 실행해서 딜레마 해결!

#### 10.1.2.4 스파크 로컬 모드

* 단일 머신에서 실행하는 독특한 형태의 스파크 자체 클러스터
* 쉽고 빠르게 구축하고 테스트하기엔 좋지만 운영 환경으로는 No
* 태스크 부하를 분산하지 않으므로 리소스가 단일 머신으로 제한됨 성능 저하

## 10.2 잡 스케줄링과 리소스 스케줄링

* 스파크 애플리케이션의 리소스 스케줄링 순서
  1. 실행자\(JVM 프로세스\)와 CPU\(태스크 슬롯\) 리소스를 스케줄링
  2. 각 실행자에 메모리 리소스 할당
* 클러스터 매니저와 스파크 스케줄러는 _스파크 잡을 실행하는 데 필요한 리소스를 부여_
  * 클러스터 매니저
    * 드라이버가 요청한 실행자 프로세스 시작
    * 클러스터 배포 모드에서 애플리케이션을 실행할 때는 드라이버 프로세스를 시작하는 역할도 담당
    * 실행 중인 프로세스를 중지하거나 재시작
    * 실행자 프로세스가 사용할 수 있는 최대 CPU 코어 개수 제한
    * 클러스터 매니저가 각 스파크 애플리케이션에 할당한 실행자들은 다른 애플리케이션과 공유되지 않음
      * 여러 스파크 애플리케이션\(및 기타 다른 유형의 애플리케이션\)을 한 클러스터에서 동시에 실행하면 애플리케이션들은 _클러스터의 리소스를 두고 경쟁_
  * 잡 스케줄링(Job scheduling)
    * 드라이버와 실행자가 시작되면 스파크 스케줄러가 이들과 직접 통신하면서 _어떤 실행자가 어떤 태스크를 수행할지 결정하는 과정_
    * 클러스터의 CPU 리소스 사용량을 좌우
      * 단일 JVM에서 실행하는 태스크가 많을수록 힙 메모리를 더 사용하므로 메모리 사용량에도 간접적으로 영향을 줌
* 스파크는 CPU 리소스와 달리 메모리 리소스는 태스크 단위로 관리하지 않음
  * 클러스터가 할당한 JVM 힙 메모리를 여러 세그먼트로 분리해 관리

즉, 스파크 애플리케이션의 리소스 스케줄링은 다음 두 가지 레벨로 이루어짐

* 클러스터 리소스 스케줄링: 여러 스파크 애플리케이션 실행자에 리소스 할당
* 스파크 리소스 스케줄링: 단일 스파크 애플리케이션 내에서 태스크를 실행할 CPU 및 메모리 리소스 스케줄링

### 10.2.1 클러스터 리소스 스케줄링

단일 클러스터에서 실행하는 다수의 애플리케이션에 클러스터의 리소스를 나누어 주는 작업

* 클러스터 매니저가 담당
* 각 애플리케이션이 요청한 리소스를 제공하고, 애플리케이션 실행이 끝나면 할당했던 리소스를 다시 회수
  * 실행자에 CPU와 메모리 리소스를 할당
* 스파크가 지원하는 모든 클러스터 유형에서 대체로 유사\(세세한 차이점은 있음\)
  * 메소스 클러스터는 미세 스케줄러\(fine-grained scheduler\)라는 독특한 기능 제공
    * 애플리케이션 단위 대신 각 태스크 단위로 리소스 할당
    * 애플리케이션이 더 이상 사용하지 않는 리소스를 다른 애플리케이션에 할당 가능

### 10.2.2 스파크 잡 스케줄링

* 클러스터 리소스 스케줄링이 끝나면 스파크 애플리케이션 내부에서 잡 스케줄링이 진행
* 클러스터 매니저와 관계없이 스파크 자체에서 수행하는 작업
  * 스파크는 RDD 계보를 바탕으로 잡과 스테이지, 태스크 생성 \(4장 참조\)
  * 잡을 어떻게 태스크로 분할하고 어떤 실행자에 전달할지 결정하고 실행 경과를 모니터링
* 스파크에서는 여러 사용자\(또는 여러 스레드\)가 동일한 `SparkContext`를 동시에 사용 가능(thread-safe 보장)하므로, 동일한 `SparkContext`를 공유하는 여러 잡은 실행자의 리소스를 두고 서로 경쟁
* 스파크는 CPU 리소스를 분배하는 방식으로 선입선출 스케줄링\(First-In-First-Out scheduling, FIFO scheduling)과 공정 스케줄링\(fair scheduling\) 모드 지원
  * `spark.scheduler.mode`를 `FIFO`나 `FAIR`로 설정

#### 10.2.2.1 선입선출 스케줄러

* 가장 먼저 리소스를 요청한 잡이 모든 실행자의 태스크 슬롯을 필요한 만큼\(또는 남은 만큼\) 전부 차지
  * 선입선출 스케줄러는 각 잡이 단일 스테이지로 구성된다고 가정
  * 예를 들어 태스크 15개를 수행해야 하는 1번 잡과 태스크 6개를 수행해야 하는 2번 잡이 있고, 태스크 슬롯을 각각 6개씩 가진 실행자가 2개 있다고 하자. 그렇다면 1번 잡이 12개의 태스크 슬롯을 모두 차지하고, 2번 잡은 6개의 태스크 슬롯만 필요하지만 1번 잡이 끝날 때까지 대기
* 스파크의 기본 스케줄링 모드로 한 번에 잡 하나만 실행하는 단일 사용자 애플리케이션에 적합

#### 10.2.2.2 공정 스케줄러

* 실행자 리소스\(즉, 실행자의 스레드\)를 놓고 경쟁하는 스파크 잡들에 라운드 로빈\(Round Robin\) 방식으로 균등하게 리소스 배분
  * 선입선출 스케줄러의 예시와 같은 상황이라면 이 두 잡의 태스크를 병렬로 실행. 즉, 1번 잡과 2번 잡은 각각 3개 씩의 태스크 슬롯을 각 실행자에서 차지
  * 실행 시간이 짧은 잡\(2번 잡\)이 태스크 슬롯을 더 늦게 요청했더라도 오래 걸리는 잡\(1번 잡\)을 완료할 때까지 기다리지 않고 바로 실행 가능
* 다중 사용자 애플리케이션이 복수의 잡을 동시에 실행하는 경우 효과적
* YARN의 큐\(12장 참조\)와 유사한 스케줄러 풀\(scheduler pool\) 기능 지원
  * 각 풀에는 가중치와 최소 지분 설정 가능
  * 가중치\(weight\): 특정 풀의 잡이 다른 풀의 잡에 비해 리소스를 더 많이 할당받을 비율을 지정
  * 최소 지분 값\(minimum share value\): 각 풀이 항상 사용할 수 있는 최소한의 CPU 코어 개수
  * `spark.scheduler.allocation.file` 매개변수에 XML 설정 파일을 지정해 설정

#### 10.2.2.3 태스크 예비 실행

* 태스크가 실행자들에 분배되는 방식을 예비 실행\(speculative execution\)이라는 개념으로 설정할 수 있음
* 예비 실행은 낙오\(straggler\) 태스크\(동일 스테이지의 다른 태스크보다 더 오래 걸리는 태스크\) 문제 해결 가능
  * 다른 프로세스가 일부 실행자 프로세스의 CPU 리소스를 모두 점유하면 해당 실행자는 태스크를 제시간에 완수하지 못할 수 있음
  * 예비 실행 기능을 사용하면 이런 경우 스파크가 해당 파티션 데이터를 처리하는 _동일한 태스크를 다른 실행자에도 요청_
  * 기존 태스크가 지연되고 예비 태스크가 완료되면 스파크는 기존 태스크의 결과 대신 예비 태스크의 결과를 사용
  * 일부 실행자의 오작동이 전체 잡의 지연으로 이어지지 않도록 방지
* 기본은 비활성화이며, `spark.speculation`을 `true`로 설정하면 사용 가능
* 예비 실행을 활성화하면 스파크는 `spark.speculation.interval`에 지정된 시간 간격마다 어떤 예비 태스크를 시작해야 할지 체크\(기본 값: 100밀리초\)
* 예비 태스크를 시작하는 기준을 별도의 매개변수로 설정 가능
  * `spark.speculation.quantile`: 스테이지가 예비 실행을 고려하기 전에 완료해야 할 태스크 진척률 지정\(기본 값: 0.75\)
  * `spark.speculation.multiplier`: 기존 태스크가 어느 정도 지연되어야 예비 태스크를 시작할지 지정\(기본 값: 1.5\)
* 일부 작업에는 예비 실행을 사용하는 것이 적절하지 않음
  * 관계형 데이터베이스와 같은 외부 시스템에 데이터를 내보내는 작업에 예비 실행을 적용하면 두 태스크가 동일 파티션의 동일 데이터를 중복 기록하는 문제 발생 가능
  * 애플리케이션에 영향을 미치는 모든 스파크 설정을 제어할 수 있는 전체 권한이 없을 때는 예비 실행을 사용하지 않는 편이 좋음
